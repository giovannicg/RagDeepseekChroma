# ğŸ§  Legal Document Analyzer con ChromaDB + Ollama + FastAPI

Este proyecto permite subir documentos legales (PDFs), extraer automÃ¡ticamente campos importantes mediante preguntas semÃ¡nticas usando embeddings con ChromaDB y un LLM local como Llama3, y chatear sobre los documentos cargados.

---

## ğŸš€ Funcionalidades

- Subida y procesamiento automÃ¡tico de PDFs
- ExtracciÃ³n automÃ¡tica de campos legales clave como:
  - `fecha_juicio`, `hora_juicio`, `demandante`, `demandado`, etc.
- Consulta vÃ­a preguntas a travÃ©s de una API (`/chat/`)
- Historial de preguntas/respuestas guardado en base de datos
- VectorizaciÃ³n y bÃºsqueda con ChromaDB
- LLM local vÃ­a Ollama

---

## ğŸ›  Requisitos

- Python 3.10+
- Docker + Docker Compose
- [Ollama](https://ollama.com/) instalado localmente y con un modelo cargado (`deepseek-coder`, `llama3`, etc.)

---
## Ejecucion
- uvicorn api.main:app --reload

## ğŸ“¡ Endpoints Principales

### `POST /upload/`
Sube un archivo PDF y lo procesa automÃ¡ticamente:
- Extrae el texto y lo divide en fragmentos (chunks)
- Genera embeddings y los almacena en ChromaDB
- Llama al modelo LLM para extraer automÃ¡ticamente campos legales como `demandante`, `cuantÃ­a`, `fecha_juicio`, etc.

**Campos del formulario**:
- `file`: Archivo PDF
- `tipo_documento`: Texto que indica el tipo (por ejemplo: "demanda")

**Respuesta**:
Devuelve un diccionario con los campos extraÃ­dos del documento.

---

### `POST /extract_info/`
Extrae automÃ¡ticamente todos los campos legales relevantes desde los documentos ya procesados, usando bÃºsqueda semÃ¡ntica.

**Respuesta**:
Un diccionario con todos los campos identificados por el modelo a partir del contexto vectorizado.

---

### `POST /chat/`
Permite hacer preguntas libres sobre el contenido de los documentos subidos, usando el LLM para generar respuestas en lenguaje natural.

**Campos**:
- `question`: Pregunta en lenguaje natural
- `numero_procedimiento` (opcional)
- `nombre_documento` (opcional)

---

### `GET /chat_history/`
Devuelve el historial de preguntas y respuestas realizadas por el usuario.

## ğŸ—ï¸ Arquitectura

```
[PDF Upload] â”€â”€â–¶ [Texto + Chunks]
                    â”‚
                    â–¼
            [ChromaDB (Embeddings)]
                    â”‚
                    â–¼
         [Modelo LLM vÃ­a Ollama (llama3)]
                    â”‚
                    â–¼
     [ExtracciÃ³n de campos + Respuestas]
```

## ğŸ—‚ï¸ Estructura del Proyecto

```
api/
â”œâ”€â”€ main.py              # Punto de entrada de la app FastAPI
â”œâ”€â”€ routes.py            # Define todos los endpoints (upload, chat, extract, etc.)
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ upload_service.py  # LÃ³gica para leer PDFs y generar chunks
â”‚   â”œâ”€â”€ chat_service.py    # InteracciÃ³n con ChromaDB y el LLM
â”‚   â””â”€â”€ extract_service.py # ExtracciÃ³n semÃ¡ntica de campos legales
â””â”€â”€ models/
    â””â”€â”€ chat_history.py    # Modelo para guardar historial de preguntas y respuestas
```