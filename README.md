# ðŸ§  Legal Document Analyzer con ChromaDB + Ollama + FastAPI

Este proyecto permite subir documentos legales (PDFs), extraer automÃ¡ticamente campos importantes mediante preguntas semÃ¡nticas usando embeddings con ChromaDB y un LLM local como Llama3, y chatear sobre los documentos cargados.

---

## ðŸš€ Funcionalidades

- Subida y procesamiento automÃ¡tico de PDFs
- ExtracciÃ³n automÃ¡tica de campos legales clave como:
  - `fecha_juicio`, `hora_juicio`, `demandante`, `demandado`, etc.
- Consulta vÃ­a preguntas a travÃ©s de una API (`/chat/`)
- Historial de preguntas/respuestas guardado en base de datos
- VectorizaciÃ³n y bÃºsqueda con ChromaDB
- LLM local vÃ­a Ollama

---

## ðŸ›  Requisitos

- Python 3.10+
- Docker + Docker Compose
- [Ollama](https://ollama.com/) instalado localmente y con un modelo cargado (`deepseek-coder`, `llama3`, etc.)

---
## Ejecucion
- uvicorn api.main:app --reload

## ðŸ“¡ Endpoints Principales

### `POST /upload/`
Sube un archivo PDF y lo procesa automÃ¡ticamente:
- Extrae el texto y lo divide en fragmentos (chunks)
- Genera embeddings y los almacena en ChromaDB
- Llama al modelo LLM para extraer automÃ¡ticamente campos legales como `demandante`, `cuantÃ­a`, `fecha_juicio`, etc.

**Campos del formulario**:
- `file`: Archivo PDF
- `tipo_documento`: Texto que indica el tipo (por ejemplo: "demanda")

**Respuesta**:
Devuelve un diccionario con los campos extraÃ­dos del documento.

---

### `POST /extract_info/`
Extrae automÃ¡ticamente todos los campos legales relevantes desde los documentos ya procesados, usando bÃºsqueda semÃ¡ntica.

**Respuesta**:
Un diccionario con todos los campos identificados por el modelo a partir del contexto vectorizado.

---

### `POST /chat/`
Permite hacer preguntas libres sobre el contenido de los documentos subidos, usando el LLM para generar respuestas en lenguaje natural.

**Campos**:
- `question`: Pregunta en lenguaje natural
- `numero_procedimiento` (opcional)
- `nombre_documento` (opcional)

---

### `GET /chat_history/`
Devuelve el historial de preguntas y respuestas realizadas por el usuario.

## Arquitectura
FastAPI â”€â”€â–º Extrae texto y genera chunks
         â”‚
         â””â”€â–º Guarda embeddings en ChromaDB
         â”‚
         â””â”€â–º Llama a Ollama (LLM local) para responder preguntas

## Estructura del Proyecto
api/
â”œâ”€â”€ routes.py              # Endpoints principales
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ upload_service.py  # Procesamiento de PDFs
â”‚   â”œâ”€â”€ chat_service.py    # BÃºsqueda + llamadas a LLM
â”‚   â””â”€â”€ extract_service.py # ExtracciÃ³n semÃ¡ntica
â”œâ”€â”€ models/
â”‚   â””â”€â”€ chat_history.py    # Modelo de historial de preguntas